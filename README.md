# The purpose of this repository
This repository contains codes for the adversarial attacks and defenses in this paper
# Adversarial Attacks
The adversarial attacks that the paper focuses on are as following:
|Name | Paper|
|----|----|
| FGSM (Linf)| Explaining and harnessing adversarial examples ([Goodfellow et al., 2014](https://arxiv.org/abs/1412.6572)) |
| BIM (Linf)| Adversarial Examples in the Physical World ([Kurakin et al., 2016](https://arxiv.org/abs/1607.02533)) |
| PGD (L2) | Towards Deep Learning Models Resistant to Adversarial Attacks ([Mardry et al., 2017](https://arxiv.org/abs/1706.06083))|
| CW (L2) | Towards Evaluating the Robustness of Neural Networks ([Carlini et al., 2016](https://arxiv.org/abs/1608.04644)) |

We generate attacks from four different image sets: CIFAR10, CIFAR100, MNIST and IMAGENET  
Each set uses 5 different backbone models: Resnet50, Resnet101, MobileNet v2, DenseNet121, AlexNet and InceptionNet v3  
The code used to generate the attacks and their top 1 and top 5 accuracies is `adversarial_attacks_get_results.py`

Usage: We index the datasets and models used as backbone in the code for convenience
* `--index_dataset` is the index of the dataset (adhering to the way we index it in the code)
* `--index_model` is the index of the model used as backbone in respect to the dataset (adhering to the way we index it in the code)
* `--father_directory` is the directory that you store the dataset, the model and the generated results (both accuracies and adversarial images)
* `--number_of_imgs` is the number of adversarial images you wish to save
* Note:
  - Datasets (along side a json file of the labels) and Models should be prepared beforehand: Most datasets can be downloaded via pytorch's Dataset, except for IMGNET. Models from pytorch are trained on IMGNET, while the code requires them to be trained on other datasets as well.
  - The code is mostly for reference purpose.
# Adversarial Defense  
There are three training methods that are reviewed in the paper:  
* ATWR: adversarial attacks generated by PGD L2 method are used to train (index number: 0)  
* ATGR: adversarial attacks generated by PGD L2 method are used to train (index number: 1)  
* EATR: adverarial attacks generated by all four methods are used to train (index number: 2)  
Usage: We also index these training methods for convenience  
* We pre-trained the models first using normal datasets before applying the methods on them. The code used is `ATWR_train.py`, `ATGR_train.py` and `EATR_train.py` respectively  
* After having finished training, the top 1 and top 5 results are taken with `adversarial_defense_get_results.py`  
* Note: The code is mostly for reference purpose.  




